name: Metal Acceleration Benchmarks

on:
  pull_request:
    branches: [main]
    paths:
      - "Sources/**/*.swift"
      - "Tests/**/*.swift"
      - ".github/workflows/metal-benchmarks.yml"

permissions:
  contents: read
  pull-requests: write

jobs:
  metal-benchmarks:
    name: Metal vs Accelerate Performance Benchmarks
    runs-on: macos-latest
    timeout-minutes: 30

    steps:
      - name: Checkout PR code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}

      - name: Setup Swift 6.1
        uses: swift-actions/setup-swift@v2
        with:
          swift-version: "6.1"

      - name: Cache Swift packages
        uses: actions/cache@v4
        with:
          path: .build
          key: ${{ runner.os }}-swift-${{ hashFiles('**/Package.resolved') }}
          restore-keys: |
            ${{ runner.os }}-swift-

      - name: Build package
        run: swift build --configuration release

      - name: Run Metal acceleration benchmarks
        id: benchmarks
        run: |
          echo "Running Metal acceleration benchmarks..."

          # Run the benchmark tests and capture output
          benchmark_output=$(swift test --filter MetalAccelerationBenchmarks --configuration release 2>&1)
          echo "$benchmark_output"

          # Extract and combine JSON results from multiple test methods
          json_blocks=$(echo "$benchmark_output" | awk '/üî¨ BENCHMARK_RESULTS_JSON_START/,/üî¨ BENCHMARK_RESULTS_JSON_END/ {if ($0 !~ /üî¨/) print}')

          if [ -n "$json_blocks" ]; then
            # Use our Python script to combine the JSON blocks
            json_results=$(echo "$json_blocks" | python3 scripts/combine_benchmark_json.py --pretty)
            
            if [ $? -eq 0 ] && [ -n "$json_results" ]; then
              echo "benchmark_results<<EOF" >> $GITHUB_OUTPUT
              echo "$json_results" >> $GITHUB_OUTPUT
              echo "EOF" >> $GITHUB_OUTPUT
              echo "benchmark_success=true" >> $GITHUB_OUTPUT
            else
              echo "Failed to combine JSON results"
              echo "benchmark_success=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "No benchmark results found"
            echo "benchmark_success=false" >> $GITHUB_OUTPUT
          fi

      - name: Checkout main branch for baseline
        if: steps.benchmarks.outputs.benchmark_success == 'true'
        uses: actions/checkout@v4
        with:
          ref: main
          path: baseline

      - name: Run baseline benchmarks
        if: steps.benchmarks.outputs.benchmark_success == 'true'
        id: baseline
        run: |
          cd baseline
          echo "Running baseline benchmarks..."

          # Build and run baseline benchmarks
          swift build --configuration release
          baseline_output=$(swift test --filter MetalAccelerationBenchmarks --configuration release 2>&1)

          # Extract baseline JSON results  
          baseline_blocks=$(echo "$baseline_output" | awk '/üî¨ BENCHMARK_RESULTS_JSON_START/,/üî¨ BENCHMARK_RESULTS_JSON_END/ {if ($0 !~ /üî¨/) print}')
          baseline_json=$(echo "$baseline_blocks" | python3 scripts/combine_benchmark_json.py --pretty)

          if [ -n "$baseline_json" ]; then
            echo "baseline_results<<EOF" >> $GITHUB_OUTPUT
            echo "$baseline_json" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
            echo "baseline_success=true" >> $GITHUB_OUTPUT
          else
            echo "baseline_success=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate benchmark report
        if: steps.benchmarks.outputs.benchmark_success == 'true'
        id: report
        run: |
          cat > benchmark_report.py << 'EOF'
          import json
          import sys
          from typing import Dict, List, Any, Optional

          def parse_benchmark_results(json_str: str) -> Optional[Dict]:
              try:
                  return json.loads(json_str)
              except json.JSONDecodeError:
                  return None

          def format_number(value: float, unit: str = "", decimals: int = 2) -> str:
              if value == 0:
                  return "N/A"
              return f"{value:.{decimals}f}{unit}"

          def calculate_speedup(metal_time: float, baseline_time: float) -> float:
              if metal_time > 0 and baseline_time > 0:
                  return baseline_time / metal_time
              return 0

          def find_optimal_batch_size(tests: List[Dict]) -> tuple:
              cosine_tests = [t for t in tests if t.get('test_type') == 'cosine_distance' and 'batch' in t.get('test_name', '')]
              
              if not cosine_tests:
                  return 32, 0  # default
              
              best_speedup = 0
              best_batch_size = 32
              
              for test in cosine_tests:
                  speedup = test.get('speedup', 0)
                  if speedup > best_speedup:
                      best_speedup = speedup
                      batch_size_str = test.get('test_name', '').split('_')[-1]
                      try:
                          best_batch_size = int(batch_size_str)
                      except ValueError:
                          pass
              
              return best_batch_size, best_speedup

          def generate_performance_summary(current_results: Dict, baseline_results: Optional[Dict] = None) -> str:
              if not current_results or 'tests' not in current_results:
                  return "‚ùå No benchmark results available"
              
              tests = current_results['tests']
              metal_available = current_results.get('metal_available', False)
              
              if not metal_available:
                  return "‚ÑπÔ∏è Metal Performance Shaders not available on this runner"
              
              # Calculate overall metrics
              total_speedup = 0
              speedup_count = 0
              memory_reductions = []
              
              cosine_tests = []
              powerset_tests = []
              e2e_tests = []
              
              for test in tests:
                  test_type = test.get('test_type', '')
                  speedup = test.get('speedup', 0)
                  
                  if speedup > 0:
                      total_speedup += speedup
                      speedup_count += 1
                  
                  if test_type == 'cosine_distance':
                      cosine_tests.append(test)
                  elif test_type == 'powerset_conversion':
                      powerset_tests.append(test)
                  elif test_type == 'end_to_end_diarization':
                      e2e_tests.append(test)
                  elif test_type == 'memory_usage':
                      memory_reduction = test.get('memory_reduction_percent', 0)
                      if memory_reduction > 0:
                          memory_reductions.append(memory_reduction)
              
              avg_speedup = total_speedup / speedup_count if speedup_count > 0 else 0
              avg_memory_reduction = sum(memory_reductions) / len(memory_reductions) if memory_reductions else 0
              optimal_batch_size, best_speedup = find_optimal_batch_size(tests)
              
              # Generate report
              report = ["## üöÄ Metal Acceleration Benchmark Results"]
              
              if avg_speedup > 0:
                  report.append(f"\n### Performance Summary")
                  report.append(f"- **Overall Average Speedup**: {format_number(avg_speedup, 'x')} faster with Metal acceleration")
                  report.append(f"- **Best Speedup Achieved**: {format_number(best_speedup, 'x')} faster")
                  report.append(f"- **Optimal Batch Size**: {optimal_batch_size} embeddings")
                  
                  if avg_memory_reduction > 0:
                      report.append(f"- **Average Memory Reduction**: {format_number(avg_memory_reduction, '%')} lower peak usage")
              
              # Detailed results table
              if cosine_tests or powerset_tests or e2e_tests:
                  report.append(f"\n### Detailed Performance Results")
                  report.append("| Operation | Configuration | Metal (ms) | Accelerate/CPU (ms) | Speedup | Memory Impact |")
                  report.append("|-----------|---------------|------------|---------------------|---------|---------------|")
                  
                  # Cosine distance results
                  for test in cosine_tests[:5]:  # Show top 5
                      name = test.get('test_name', '').replace('cosine_distance_', '')
                      config = f"{test.get('num_queries', 0)}√ó{test.get('num_candidates', 0)} ({test.get('embedding_dim', 0)}d)"
                      metal_time = format_number(test.get('metal_time_ms', 0))
                      accel_time = format_number(test.get('accelerate_time_ms', 0))
                      speedup = format_number(test.get('speedup', 0), 'x')
                      memory = format_number(test.get('memory_increase_mb', 0), 'MB')
                      report.append(f"| Cosine Distance ({name}) | {config} | {metal_time} | {accel_time} | {speedup} | {memory} |")
                  
                  # Powerset conversion results
                  for test in powerset_tests[:3]:  # Show top 3
                      name = test.get('test_name', '').replace('powerset_', '')
                      config = f"{test.get('batch_size', 0)} batch, {test.get('num_frames', 0)} frames"
                      metal_time = format_number(test.get('metal_time_ms', 0))
                      cpu_time = format_number(test.get('cpu_time_ms', 0))
                      speedup = format_number(test.get('speedup', 0), 'x')
                      throughput = format_number(test.get('throughput_frames_per_sec', 0), ' fps')
                      report.append(f"| Powerset Conv ({name}) | {config} | {metal_time} | {cpu_time} | {speedup} | {throughput} |")
                  
                  # End-to-end results
                  for test in e2e_tests:
                      duration = test.get('audio_duration_seconds', 0)
                      config = f"{duration}s audio"
                      metal_time = format_number(test.get('metal_time_ms', 0))
                      accel_time = format_number(test.get('accelerate_time_ms', 0))
                      speedup = format_number(test.get('speedup', 0), 'x')
                      rtf = format_number(test.get('real_time_factor', 0), 'x RT')
                      report.append(f"| End-to-End Diarization | {config} | {metal_time} | {accel_time} | {speedup} | {rtf} |")
              
              # Recommendations
              if avg_speedup > 0:
                  report.append(f"\n### Recommendations")
                  if avg_speedup >= 2.0:
                      report.append("‚úÖ **Excellent performance improvement** - Metal acceleration is highly beneficial")
                  elif avg_speedup >= 1.5:
                      report.append("‚úÖ **Good performance improvement** - Metal acceleration provides solid benefits")
                  elif avg_speedup >= 1.2:
                      report.append("‚ö†Ô∏è **Moderate performance improvement** - Metal acceleration provides some benefits")
                  else:
                      report.append("‚ö†Ô∏è **Limited performance improvement** - Consider optimizing Metal implementation")
                  
                  report.append(f"- Use batch size of **{optimal_batch_size}** for optimal performance")
                  report.append("- Metal acceleration is most beneficial for large embedding matrices")
                  
                  if avg_memory_reduction > 10:
                      report.append(f"- Metal also provides {format_number(avg_memory_reduction, '%')} memory efficiency improvement")
              
              # Performance regression check
              if baseline_results and baseline_results.get('tests'):
                  baseline_tests = baseline_results['tests']
                  baseline_speedups = [t.get('speedup', 0) for t in baseline_tests if t.get('speedup', 0) > 0]
                  baseline_avg = sum(baseline_speedups) / len(baseline_speedups) if baseline_speedups else 0
                  
                  if baseline_avg > 0:
                      regression = ((baseline_avg - avg_speedup) / baseline_avg) * 100
                      if regression > 10:
                          report.append(f"\n‚ö†Ô∏è **Performance Regression Detected**: {format_number(regression, '%')} slower than baseline")
                      elif regression < -10:
                          report.append(f"\nüéâ **Performance Improvement**: {format_number(-regression, '%')} faster than baseline")
              
              return '\n'.join(report)

          def main():
              if len(sys.argv) < 2:
                  print("Error: No benchmark results provided")
                  return
              
              current_results = parse_benchmark_results(sys.argv[1])
              baseline_results = None
              
              if len(sys.argv) >= 3:
                  baseline_results = parse_benchmark_results(sys.argv[2])
              
              report = generate_performance_summary(current_results, baseline_results)
              print(report)

          if __name__ == "__main__":
              main()
          EOF

          # Generate the report
          python3 benchmark_report.py '${{ steps.benchmarks.outputs.benchmark_results }}' '${{ steps.baseline.outputs.baseline_results }}' > benchmark_report.md

          # Output the report for the comment step
          echo "report<<EOF" >> $GITHUB_OUTPUT
          cat benchmark_report.md >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Comment PR with benchmark results
        if: steps.benchmarks.outputs.benchmark_success == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const report = `${{ steps.report.outputs.report }}`;

            // Find existing benchmark comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.data.find(comment => 
              comment.user.login === 'github-actions[bot]' && 
              comment.body.includes('üöÄ Metal Acceleration Benchmark Results')
            );

            const commentBody = report + `\n\n---\n*Benchmarks run on: \`${context.payload.pull_request.head.sha.slice(0, 7)}\` vs \`main\`*`;

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: commentBody
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: commentBody
              });
            }

      - name: Check for performance regressions
        if: steps.benchmarks.outputs.benchmark_success == 'true'
        run: |
          # Parse benchmark results to check for significant regressions
          python3 -c "
          import json
          import sys

          try:
              results = json.loads('${{ steps.benchmarks.outputs.benchmark_results }}')
              tests = results.get('tests', [])
              
              # Check for any major performance regressions
              major_regressions = []
              for test in tests:
                  speedup = test.get('speedup', 0)
                  test_name = test.get('test_name', '')
                  
                  # Flag tests with very poor performance (less than 1.1x speedup for large operations)
                  if 'large' in test_name or 'scale' in test_name:
                      if 0 < speedup < 1.1:
                          major_regressions.append(f'{test_name}: {speedup:.2f}x speedup')
              
              if major_regressions:
                  print('Performance regression detected in:')
                  for regression in major_regressions:
                      print(f'  - {regression}')
                  sys.exit(1)
              else:
                  print('No significant performance regressions detected')
          except Exception as e:
              print(f'Error checking regressions: {e}')
          "

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: metal-benchmark-results
          path: |
            benchmark_report.md
          retention-days: 30
