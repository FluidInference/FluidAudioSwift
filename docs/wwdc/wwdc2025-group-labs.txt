At WWDC25 we launched a new type of Lab event for the developer community - Group Labs. A Group Lab is a panel Q&A designed for a large audience of developers. Group Labs are a unique opportunity for the community to submit questions directly to a panel of Apple engineers and designers. Here are the highlights from the WWDC25 Group Lab for Design.

Can you expand on how Liquid Glass helps with navigation and focus in the UI?

Liquid Glass clarifies the navigation layer by introducing a single, floating pane that acts as the primary navigation area. Buttons within this pane seamlessly morph as you move between sections, and controls can temporarily lift into the glass surface. While avoiding excessive use of glass (like layering glass on glass), this approach simplifies navigation and strengthens the connection between menus, alerts, and the elements that trigger them.

What should I do with customized bars that I might have in my app?

Reconsider the content and behavior of customized bars. Evaluate whether you need all the buttons and whether a menu might be a better solution. Instead of relying on background colors or styling, express hierarchy through layout and grouping. This is a good opportunity to adopt the new design language and simplify your interface.

What are scroll edge effects, and what options do we have for them?

Scroll edge effects enhance legibility in controls by lifting interactive elements and separating them from the background. There are two types: a soft edge effect (a subtle blur) and a hard edge effect (a more defined boundary for high-legibility areas like column sorting). Scroll edge effects are designed to work seamlessly with Liquid Glass, allowing content to feel expansive while ensuring controls and titles remain legible.

How can we ensure or improve accessibility using Liquid Glass?

Legibility is a priority, and refinements are ongoing throughout the betas. Liquid Glass adapts well to accessibility settings like Reduce Transparency, Increase Contrast, and Reduce Motion. There are two variants of glass: regular glass, designed to be legible by default, and clear glass, used in places like AVKit, which requires more care to ensure legibility. Use color contrast tools to ensure contrast ratios are met. The Human Interface Guidelines (HIG) are a living document offering best practices. The colors and materials pages are key resources.

Do you have any recommendations for convincing designers concerned with consistency across Android and Web to use Liquid Glass?

Start small and focus on high-utility controls that don't significantly impact brand experience. Native controls offer familiarity and predictability to users. Using the native controls makes sure your app feels at home on the device. Using native frameworks provides built-in accessibility support (dynamic type, reduce transparency, increase contrast). Native controls come with built-in behaviors and interactions.

Can ScrollViews include Liquid Glass within them?

You can technically put a glass layer inside a scroll view, but it can feel heavy and doesn't align with the system's intention for Liquid Glass to serve as a fixed layer. Think of the content layer as the scrolling layer, and the navigational layer as the one using Liquid Glass. If there is glass on the content layer it will collide into the navigational layer.

What core design philosophy guided the direction of iOS 26, beyond the goal of unification?

The core design philosophy involved blurring the line between hardware and software, separating UI and navigation elements from content, making apps adaptable across window sizes, and combining playfulness with sophistication. It was about making the UI feel at home on rounded screens.

Can we layer Liquid Glass elements on top of each other?

Avoid layering Liquid Glass elements directly on top of each other, as it creates unnecessary visual complexity. The system will automatically convert nested glass elements to a vibrant fill style. Use vibrant fills and labels to show control shapes and ensure legibility. Opaque grays should be avoided in favor of vibrant colors, which will multiply with the backgrounds correctly.

What will happen to apps that use custom components? Should they be adapted to the new design within the next year?

The more native components you use, the more things happen for free. Standard components will be upgraded automatically. Look out for any customizations that might clash. Think about what is the minimum viable change, where your app still feels and looks very similar to what it did. Prioritize changes in core workflows and navigational areas. There are a number of benefits to using native components including user familiarity, built-in accessibility support, and built-in behaviors and interactions.

Will Apple be releasing Figma design templates?

Sketch kits were published on Monday and can be referenced. The goal is to ensure the resources are well-organized, well-named, and easy to use. It's a high priority.


At WWDC25 we launched a new type of Lab event for the developer community - Group Labs. A Group Lab is a panel Q&A designed for a large audience of developers. Group Labs are a unique opportunity for the community to submit questions directly to a panel of Apple engineers and designers. Here are the highlights from the WWDC25 Group Lab for Design.

Can you expand on how Liquid Glass helps with navigation and focus in the UI?

Liquid Glass clarifies the navigation layer by introducing a single, floating pane that acts as the primary navigation area. Buttons within this pane seamlessly morph as you move between sections, and controls can temporarily lift into the glass surface. While avoiding excessive use of glass (like layering glass on glass), this approach simplifies navigation and strengthens the connection between menus, alerts, and the elements that trigger them.

What should I do with customized bars that I might have in my app?

Reconsider the content and behavior of customized bars. Evaluate whether you need all the buttons and whether a menu might be a better solution. Instead of relying on background colors or styling, express hierarchy through layout and grouping. This is a good opportunity to adopt the new design language and simplify your interface.

What are scroll edge effects, and what options do we have for them?

Scroll edge effects enhance legibility in controls by lifting interactive elements and separating them from the background. There are two types: a soft edge effect (a subtle blur) and a hard edge effect (a more defined boundary for high-legibility areas like column sorting). Scroll edge effects are designed to work seamlessly with Liquid Glass, allowing content to feel expansive while ensuring controls and titles remain legible.

How can we ensure or improve accessibility using Liquid Glass?

Legibility is a priority, and refinements are ongoing throughout the betas. Liquid Glass adapts well to accessibility settings like Reduce Transparency, Increase Contrast, and Reduce Motion. There are two variants of glass: regular glass, designed to be legible by default, and clear glass, used in places like AVKit, which requires more care to ensure legibility. Use color contrast tools to ensure contrast ratios are met. The Human Interface Guidelines (HIG) are a living document offering best practices. The colors and materials pages are key resources.

Do you have any recommendations for convincing designers concerned with consistency across Android and Web to use Liquid Glass?

Start small and focus on high-utility controls that don't significantly impact brand experience. Native controls offer familiarity and predictability to users. Using the native controls makes sure your app feels at home on the device. Using native frameworks provides built-in accessibility support (dynamic type, reduce transparency, increase contrast). Native controls come with built-in behaviors and interactions.

Can ScrollViews include Liquid Glass within them?

You can technically put a glass layer inside a scroll view, but it can feel heavy and doesn't align with the system's intention for Liquid Glass to serve as a fixed layer. Think of the content layer as the scrolling layer, and the navigational layer as the one using Liquid Glass. If there is glass on the content layer it will collide into the navigational layer.

What core design philosophy guided the direction of iOS 26, beyond the goal of unification?

The core design philosophy involved blurring the line between hardware and software, separating UI and navigation elements from content, making apps adaptable across window sizes, and combining playfulness with sophistication. It was about making the UI feel at home on rounded screens.

Can we layer Liquid Glass elements on top of each other?

Avoid layering Liquid Glass elements directly on top of each other, as it creates unnecessary visual complexity. The system will automatically convert nested glass elements to a vibrant fill style. Use vibrant fills and labels to show control shapes and ensure legibility. Opaque grays should be avoided in favor of vibrant colors, which will multiply with the backgrounds correctly.

What will happen to apps that use custom components? Should they be adapted to the new design within the next year?

The more native components you use, the more things happen for free. Standard components will be upgraded automatically. Look out for any customizations that might clash. Think about what is the minimum viable change, where your app still feels and looks very similar to what it did. Prioritize changes in core workflows and navigational areas. There are a number of benefits to using native components including user familiarity, built-in accessibility support, and built-in behaviors and interactions.

Will Apple be releasing Figma design templates?

Sketch kits were published on Monday and can be referenced. The goal is to ensure the resources are well-organized, well-named, and easy to use. It's a high priority.

At WWDC25 we launched a new type of Lab event for the developer community - Group Labs. A Group Lab is a panel Q&A designed for a large audience of developers. Group Labs are a unique opportunity for the community to submit questions directly to a panel of Apple engineers and designers. Here are the highlights from the WWDC25 Group Lab for SwiftUI.

What's your favorite new feature introduced to SwiftUI this year?

The new rich text editor, a collaborative effort across multiple Apple teams.
The safe area bar, simplifying the management of scroll view insets, safe areas, and overlays.
NavigationLink indicator visibility control, a highly requested feature now available and back-deployed.
Performance improvements to existing components (lists, scroll views, etc.) that come "for free" without requiring API adoption.
Regarding performance profiling, it's recommended to use the new SwiftUI Instruments tool when you have a good understanding of your code and notice a performance drop after a specific change. This helps build a mental map between your code and the profiler's output. The "cause-and-effect graph" in the tool is particularly useful for identifying what's triggering expensive view updates, even if the issue isn't immediately apparent in your own code.

My app is primarily UIKit-based, but I'm interested in adopting some newer SwiftUI-only scene types like MenuBarExtra or using SwiftUI-exclusive features. Is there a better way to bridge these worlds now?

Yes, "scene bridging" makes it possible to use SwiftUI scenes from UIKit or AppKit lifecycle apps. This allows you to display purely SwiftUI scenes from your existing UIKit/AppKit code. Furthermore, you can use SwiftUI scene-specific modifiers to affect those scenes. Scene bridging is a great way to introduce SwiftUI into your apps. This also allows UIKit apps brought to Vision OS to integrate volumes and immersive spaces. It's also a great way to customize your experience with Assistive Access API.

Can you please share any bad practices we should avoid when integrating Liquid Glass in our SwiftUI Apps?

Avoid these common mistakes when integrating liquid glass:

Overlapping Glass: Don't overlap liquid glass elements, as this can create visual artifacts.

Scrolling Content Collisions: Be cautious when using liquid glass within scrolling content to prevent collisions with toolbar and navigation bar glass.

Unnecessary Tinting: Resist the urge to tint the glass for branding or other purposes. Liquid glass should primarily be used to draw attention and convey meaning.

Improper Grouping: Use the GlassEffectContainer to group related glass elements. This helps the system optimize rendering by limiting the search area for glass interactions.

Navigation Bar Tinting: Avoid tinting navigation bars for branding, as this conflicts with the liquid glass effect. Instead, move branding colors into the content of the scroll view. This allows the color to be visible behind the glass at the top of the view, but it moves out of the way as the user scrolls, allowing the controls to revert to their standard monochrome style for better readability.

Thanks for improving the performance of SwiftUI List this year. How about LazyVStack in ScrollView? Does it now also reuse the views inside the stack? Are there any best practices for improving the performance when using LazyVStack with large number of items?

SwiftUI has improved scroll performance, including idle prefetching. When using LazyVStack with a large number of items, ensure your ForEach returns a static number of views. If you're returning multiple views within the ForEach, wrap them in a VStack to ****** to SwiftUI that it's a single row, allowing for optimizations. Reuse is handled as an implementation detail within SwiftUI. Use the performance instrument to identify expensive views and determine how to optimize your app. If you encounter performance issues or hitches in scrolling, use the new SwiftUI Instruments tool to diagnose the problem.

Implementing the new iOS 26 tab bar seems to have very low contrast when darker content is underneath, is there anything we should be doing to increase the contrast for tab bars?

The new design is still in beta. If you're experiencing low contrast issues, especially with darker content underneath, please file feedback. It's generally not recommended to modify standard system components. As all apps on the platform are adopting liquid glass, feedback is crucial for tuning the experience based on a wider range of apps. Early feedback, especially regarding contrast and accessibility, is valuable for improving the system for all users.

If I’m starting a new multi-platform app (iOS/iPadOS/macOS) that will heavily depend on UIKit/AppKit for the core structure and components (split, collection, table, and outline views), should I still use SwiftUI to manage the app lifecycle? Why?

Even if your new multi-platform app heavily relies on UIKit/AppKit for core structure and components, it's generally recommended to still use SwiftUI to manage the app lifecycle. This sets you up for easier integration of SwiftUI components in the future and allows you to quickly adopt new SwiftUI features. Interoperability between SwiftUI and UIKit/AppKit is a core principle, with APIs to facilitate going back and forth between the two frameworks. Scene bridging allows you to bring existing SwiftUI scenes into apps that use a UIKit lifecycle, or vice versa. Think of it not as a binary choice, but as a mix of whatever you need.

I’d love to know more about the matchedTransitionSource API you’ve added - is it a native way to have elements morph from a VStack to a sheet for example? What is the use case for it?

The matchedTransitionSource API helps connect different views during transitions, such as when presenting popovers or other presentations from toolbar items. It's a way to link the user interaction to the presented content. For example, it can be used to visually connect an element in a VStack to a sheet. It can also be used to create a zoom effect where an element appears to enlarge, and these transitions are fully interactive, allowing users to swipe. It creates a nice, polished experience for the user. Support for this API has been added to toolbar items this year, and it was already available for standard views.

Instance singleton vs EnvironmentObject for ViewModels?

While instance singletons (public static let shared) are common, especially from UIKit/AppKit development, @EnvironmentObject is preferred in SwiftUI. Singletons can make your code more difficult to test. The @EnvironmentObject property wrapper solves the problem of accessing a shared object by offering a convenient mechanism for sharing model data across your app, guaranteeing that views remain synchronized with the latest data. While singletons aren't actively harmful, they can make refactoring more difficult. @EnvironmentObject also allows you to inject different contexts to see different previews, too.

What’s the best way in SwiftUI for a View to communicate values back to its parent that cannot be overridden by the parent? Binding is not really suitable because it enables the parent to modify the values.

If you need a child View to communicate values back to its parent without allowing the parent to modify them, Binding is not suitable. Consider these approaches:

Container Values: If the parent is a container, explore container values. These values can be read by the container.

Preferences: If you need to communicate values upwards, consider using preferences. Preferences flow up the view hierarchy and you can combine multiple preferences.

Closures: Use closures, especially for performing actions or when interacting with UIKit/AppKit views.

For an example see the code in the Cook up a rich text experience in SwiftUI with AttributedString session, which demonstrates using a preference to pass the current selection up the view hierarchy.

I saw that animations are available in Widgets for visionOS. Will it also be available for widgets on iOS?

The same animation capabilities available for widgets on visionOS have been supported on iOS since iOS 17. For more information on animated and interactive widgets, refer to the Bring Widgets to Life session from WWDC 23.

Is it possible to add hyperlinks in the new rich text editor?

Yes, it is possible to add hyperlinks in the new rich text editor. Use the Foundation's link attribute. Please be aware that in seed one there's a known issue where gestures aren't behaving correctly with links.

Whats the best way to implement custom tab bars in SwiftUI?

Before implementing a fully custom tab bar, consider whether you can achieve your desired result using the standard system controls. There are APIs to customize aspects like color, labels, and even add a separate search tab. Using system controls ensures a consistent appearance across different operating systems. If a more custom design is necessary, explore the new glass APIs like GlassEffectContainer, GlassEffect, and SafeAreaBar. Please submit feedback if a missing feature prevents you from using the standard tab bar.

Is there an API to show the navigation link chevron when there isn’t a navigation link but a button?

There isn't a direct API, no. However, you can achieve a similar effect by:

Using an environment key for indicatorVisibility.
Using the trailing chevron SF Symbol, which is what navigation links are styled as.
Replicating the NavigationLink selection state.
Can you create custom components like the tab bar with a draggable glass droplet?

You can build custom liquid glass components using the new GlassEffect API. However, there isn't currently a specific API to replicate the "lift up" effect seen when changing selections in the system tab bar. If you have use cases requiring additional APIs for creating such effects in your custom controls, please file feedback. Providing detailed feedback with screenshots and descriptions of your use case is valuable for informing the design of future API, helping the team create simple, flexible, and composable solutions.

What’s the recommended way to display thousands of thumbnails in a LazyVGrid without dropping frames or showing empty cells while the thumbnail loads?

To display thousands of thumbnails in a LazyVGrid without dropping frames or showing empty cells:

Concurrency: Move work off the main thread. The Embracing Swift Concurrency session from WWDC25 provides guidance. The SwiftUI performance instrument can help identify issues caused by main thread work. Also see the Code-along: Elevate an App with Swift Concurrency session.

Image Scaling: Downscale high-resolution images to the actual display size.

Performance Instrument: Use this new instrument to identify expensive view initializers or bodies.

Invalidation Debugging: Use this debugging trick - set a random background color on the photo insets to visualize cell invalidation during scrolling. A "disco ball" effect indicates excessive invalidation.

Is there any way to make DatePicker respect Dynamic Type? It really seems to like rendering itself at a small size no matter what I do.

This is a known issue and unfortunately there is not a way to customize it.


At WWDC25 we launched a new type of Lab event for the developer community - Group Labs. A Group Lab is a panel Q&A designed for a large audience of developers. Group Labs are a unique opportunity for the community to submit questions directly to a panel of Apple engineers and designers. Here are the highlights from the WWDC25 Group Lab for UI Frameworks.

How would you recommend developers start adopting the new design?

Start by focusing on the foundational structural elements of your application, working from the "top down" or "bottom up" based on your application's hierarchy. These structural changes, like edge-to-edge content and updated navigation and controls, often require corresponding code modifications. As a first step, recompile your application with the new SDK to see what updates are automatically applied, especially if you've been using standard controls. Then, carefully analyze where the new design elements can be applied to your UI, paying particular attention to custom controls or UI that could benefit from a refresh. Address the large structural items first then focus on smaller details is recommended.

Will we need to migrate our UI code to Swift and SwiftUI to adopt the new design?

No, you will not need to migrate your UI code to Swift and SwiftUI to adopt the new design. The UI frameworks fully support the new design, allowing you to migrate your app with as little effort as possible, especially if you've been using standard controls. The goal is to make it easy to adopt the new design, regardless of your current UI framework, to achieve a cohesive look across the operating system.

What was the reason for choosing Liquid Glass over frosted glass, as used in visionOS?

The choice of Liquid Glass was driven by the desire to bring content to life. The see-through nature of Liquid Glass enhances this effect. The appearance of Liquid Glass adapts based on its size; larger glass elements look more frosted, which aligns with the design of visionOS, where everything feels larger and benefits from the frosted look.

What are best practices for apps that use customized navigation bars?

The new design emphasizes behavior and transitions as much as static appearance. Consider whether you truly need a custom navigation bar, or if the system-provided controls can meet your needs. Explore new APIs for subtitles and custom views in navigation bars, designed to support common use cases. If you still require a custom solution, ensure you're respecting safe areas using APIs like SwiftUI's safeAreaInset. When working with Liquid Glass, group related buttons in shared containers to maintain design consistency. Finally, mark glass containers as interactive.

For branding, instead of coloring the navigation bar directly, consider incorporating branding colors into the content area behind the Liquid Glass controls. This creates a dynamic effect where the color is visible through the glass and moves with the content as the user scrolls.

I want to know why new UI Framework APIs aren’t backward compatible, specifically in SwiftUI? It leads to code with lots of if-else statements.

Existing APIs have been updated to work with the new design where possible, ensuring that apps using those APIs will adopt the new design and function on both older and newer operating systems. However, new APIs often depend on deep integration across the framework and graphics stack, making backward compatibility impractical.

When using these new APIs, it's important to consider how they fit within the context of the latest OS. The use of if-else statements allows you to maintain compatibility with older systems while taking full advantage of the new APIs and design features on newer systems. If you are using new APIs, it likely means you are implementing something very specific to the new design language. Using conditional code allows you to intentionally create different code paths for the new design versus older operating systems. Prefer to use if #available where appropriate to intentionally adopt new design elements.

Are there any Liquid Glass materials in iOS or macOS that are only available as part of dedicated components? Or are all those materials available through new UIKit and AppKit views?

Yes, some variations of the Liquid Glass material are exclusively available through dedicated components like sliders, segmented controls, and tab bars. However, the "regular" and "clear" glass materials should satisfy most application requirements. If you encounter situations where these options are insufficient, please file feedback.

If I were to create an app today, how should I design it to make it future proof using Liquid Glass?

The best approach to future-proof your app is to utilize standard system controls and design your UI to align with the standard system look and feel. Using the framework-provided declarative API generally leads to easier adoption of future design changes, as you're expressing intent rather than specifying pixel-perfect visuals. Pay close attention to the design sessions offered this year, which cover the design motivation behind the Liquid Glass material and best practices for its use.

Is it possible to implement your own sidebar on macOS without NSSplitViewController, but still provide the Liquid Glass appearance?

While technically possible to create a custom sidebar that approximates the Liquid Glass appearance without using NSSplitViewController, it is not recommended. The system implementation of the sidebar involves significant unseen complexity, including interlayering with scroll edge effects and fullscreen behaviors. NSSplitViewController provides the necessary level of abstraction for the framework to handle these details correctly.

Regarding the SceneDelagate and scene based life-cycle, I would like to confirm that AppDelegate is not going away. Also if the above is a correct understanding, is there any advice as to what should, and should not, be moved to the SceneDelegate?

UIApplicationDelegate is not going away and still serves a purpose for application-level interactions with the system and managing scenes at a higher level. Move code related to your app's scene or UI into the UISceneDelegate. Remember that adopting scenes doesn't necessarily mean supporting multiple scenes; an app can be scene-based but still support only one scene. Refer to the tech note Migrating to the UIKit scene-based life cycle and the Make your UIKit app more flexible WWDC25 session for more information.


At WWDC25 we launched a new type of Lab event for the developer community - Group Labs. A Group Lab is a panel Q&A designed for a large audience of developers. Group Labs are a unique opportunity for the community to submit questions directly to a panel of Apple engineers and designers. Here are the highlights from the WWDC25 Group Lab for Developer Tools.

Will my project codebase be used for training when I use Xcode's intelligent assistant powered by cloud-based models?

When using ChatGPT without logging in, your data will not be used to improve any models. If you log in to a ChatGPT account, this is based on your ChatGPT account settings, which allows you to opt-out (it defaults to on). When using Xcode with accounts for other model providers, you should check with the policies of your provider. And finally, at no point will any portion of your codebase be used to train or improve any Apple models.

We'd love to make our SwiftUI Previews (and soon, Playgrounds) as snappy as possible. Is there any way to skip certain build steps, such as running linters? It seems the build environment is exactly the same (compared to a debug build), but maybe there's a trick.

Starting with Xcode 16, SwiftUI previews use the exact same build artifacts as the regular build. The new Playgrounds support in Xcode 26 uses these build artifacts too. Shell script build phases are the most common thing that introduces extra build time, so as a first step, try turning off all shell script build phases (like linters) to get an idea if that’s the issue. If those build phases add significant time to your build, consider moving some of those phases into asynchronous steps, such as running linters before committing instead of on every build. If you do need a shell script build phase to run during your build, make sure to explicitly define the input and output files, as that is a huge way to improve your build performance.

Are we able to provide additional context for the models, like coding standards? Documentation for third party dependencies? Documentation on your own codebase that explains things like architecture and more?

In general, Xcode will automatically search for the right context based on the question and the evolving answer, as the model can interact multiple times with your project as it develops an answer. This will automatically pick up the coding style of the code it sees, and can include files that contain architecture comments, etc. Beyond automatic context, you can manually attach other documents, even if they aren't in your project. For example, you could make a file with rules and ideas and attach it, and it will influence the response. We are very aware of other kinds of automatic context like rule files, etc, though Xcode does not support these at this time.

Once ChatGPT is enabled for Coding Intelligence in Xcode 26, and I sign into my existing ChatGPT account, will the ChatGPT Coding Intelligence model in Xcode know about chat conversations on Xcode development done previously in the ChatGPT Mac app?

Xcode does not use information from other conversations, and conversations started in Xcode are not accessible in the web UI or ChatGPT app.

Is there a plan to make SwiftUI views easier to locate and understand in the view hierarchy like UIKit views?

SwiftUI uses a declarative paradigm to define your user interface. That allows you to specify what you want, with the system translating that into an efficient representation at runtime. Unlike traditional AppKit and UIKit, seeing the runtime representation of SwiftUI views isn't sufficient in order to understand why it's not doing what you want. This year, we introduced a SwiftUI Instrument that shows why things are happening, like view re-rendering.

Is it possible to use the AI chat with ChatGPT Enterprise? My company doesn't allow us to use the general ChatGPT, only the enterprise version they have setup that prevents data from being leaked

Yes, Xcode 26 supports logging into any existing ChatGPT account, including enterprise accounts. If that does not meet your needs, you can also setup a local server that implements the popular chat completions REST API to talk to your enterprise account how you need.

Now that Icon Composer is here, how does it complement or replace existing vector design tools such as Sketch for icon design?

Icon Composer complements your existing vector design tools. You should continue to create your shapes, gradients, and layers in another tool like Sketch, and compose the exported SVG layers in Icon Composer. Once you bring your layers into Icon Composer, you can then use it to influence the translucency, blur, and specular highlights for your icon.

What’s one feature or improvement in the new Xcode that you personally think developers will love, but might not immediately discover? Maybe something tucked away or quietly powerful that’s flown under the radar so far?

One feature we're particularly excited about is the new power profiler for iOS, which gives you further insights into the energy consumption of your app beyond what was possible with the energy instrument previously. You can learn more about how to use this instrument and how it can help you greatly reduce your apps battery usage in the documentation, as well as the session Profile and optimize power usage in your app. There were also improvements in accessibility this year with Voice Control, where you can naturally speak your Swift code to Xcode, and it understands the Swift syntax as you speak. To see it in action, take a look at the demonstration in What’s new in Xcode 26.

We have a software advisory council that is very sensitive to having our private information going to the cloud in any form. What information do you have to help me guide Xcode and Apple Intelligence through the acceptance process?

One thing you can do is configure a proxy for your enterprise that implementing the popular Chat Completions API endpoint protocol. When using a model provider via URL, you can use your proxy endpoint to inspect the network traffic for anything that you do not want sent outside of your enterprise, and then forward the traffic through the proxy to your chosen model provider.

Are there list of recommended LLMs to use with Xcode via Intelligence/Local? I've tried Gemma3-12B, but.. I hope there are better options?

Apple doesn't have a published list of recommended local models. This is a fast-moving space, and so a recommendation would become out of date very quickly as new models are released. We encourage you to try out the local model support in Xcode 26 with models that you find meet your needs, and let us and the community know!

Swift Testing is great for unit tests, but I was hoping there would be some love this year to improve the way UI tests are written and structured too. Is this something we can expect this year?

Xcode 26 contains improvements to how you can record the actions for your UI test, and select the element queries that drive the UI test through the XCTest APIs. Our session Record, replay, and review: UI automation with Xcode is a great resource to understand the new improvements to these testing workflows.

Are the new Icon Composer files backward compatible with previous OSes and Xcode will take care of that or do we need to also keep the old AppIcon?

Yes, icons made with Icon Composer will automatically create a pre-composited bitmap icon during the build process that supports previous OS versions.

Are there new debugging tools in Xcode that help developing with Swift Concurrency?

Yes, there are lots of improvements under the hood in Xcode 26! They include following async tasks across threads when stepping in LLDB and Swift task names appearing in the thread view. LLDB also now has data formatters for Swift Task and TaskGroup, which allow you to look at the Task hierarchy in the Xcode variable view.

I've been building applications using Objective-C. What does the future look like for building Objective-C projects in Xcode 26 and beyond?

You can still build Objective-C code — it's not going away! We definitely believe in modern, memory-safe languages like Swift, and it's easier than ever to mix+match Swift with Objective-C. Further, the new intelligence features provide new opportunities for incorporating new techniques into existing codebases.

Yesterday, we learned that macOS Tahoe will be the last to support Intel Macs. What should we expect regarding developer tools once macOS is Apple silicon only? Are there any preparations we should make so we can still ship updates of our universal apps that also support older OSes on Intel?

The transition to Apple silicon is complete, and so we’re focused on the future. With Xcode 26, you can still build and ship Universal apps with Intel slices. We know many apps deploy to older OS versions that support Intel-based Macs, and support for distributing Universal binaries to Intel-based Macs running older OS versions is not going away.

Can you tell us more about Swift Compilation cache?

The Swift compilation cache is a local cache that improves build performance by providing prior compilation results directly from the cache. It's useful when switching between branches and you need to recompile your code. It supports both the Swift compiler and Clang. You can opt in using the Enable Compilation Caching build setting.

At WWDC25 we launched a new type of Lab event for the developer community - Group Labs. A Group Lab is a panel Q&A designed for a large audience of developers. Group Labs are a unique opportunity for the community to submit questions directly to a panel of Apple engineers and designers. Here are the highlights from the WWDC25 Group Lab for Machine Learning and AI Frameworks.

What are you most excited about in the Foundation Models framework?

The Foundation Models framework provides access to an on-device Large Language Model (LLM), enabling entirely on-device processing for intelligent features. This allows you to build features such as personalized search suggestions and dynamic NPC generation in games. The combination of guided generation and streaming capabilities is particularly exciting for creating delightful animations and features with reliable output. The seamless integration with SwiftUI and the new design material Liquid Glass is also a major advantage.

When should I still bring my own LLM via CoreML?

It's generally recommended to first explore Apple's built-in system models and APIs, including the Foundation Models framework, as they are highly optimized for Apple devices and cover a wide range of use cases. However, Core ML is still valuable if you need more control or choice over the specific model being deployed, such as customizing existing system models or augmenting prompts. Core ML provides the tools to get these models on-device, but you are responsible for model distribution and updates.

Should I migrate PyTorch code to MLX?

MLX is an open-source, general-purpose machine learning framework designed for Apple Silicon from the ground up. It offers a familiar API, similar to PyTorch, and supports C, C++, Python, and Swift. MLX emphasizes unified memory, a key feature of Apple Silicon hardware, which can improve performance. It's recommended to try MLX and see if its programming model and features better suit your application's needs. MLX shines when working with state-of-the-art, larger models.

Can I test Foundation Models in Xcode simulator or device?

Yes, you can use the Xcode simulator to test Foundation Models use cases. However, your Mac must be running macOS Tahoe. You can test on a physical iPhone running iOS 18 by connecting it to your Mac and running Playgrounds or live previews directly on the device.

Which on-device models will be supported? any open source models?

The Foundation Models framework currently supports Apple's first-party models only. This allows for platform-wide optimizations, improving battery life and reducing latency. While Core ML can be used to integrate open-source models, it's generally recommended to first explore the built-in system models and APIs provided by Apple, including those in the Vision, Natural Language, and Speech frameworks, as they are highly optimized for Apple devices. For frontier models, MLX can run very large models.

How often will the Foundational Model be updated? How do we test for stability when the model is updated?

The Foundation Model will be updated in sync with operating system updates. You can test your app against new model versions during the beta period by downloading the beta OS and running your app. It is highly recommended to create an "eval set" of golden prompts and responses to evaluate the performance of your features as the model changes or as you tweak your prompts. Report any unsatisfactory or satisfactory cases using Feedback Assistant.

Which on-device model/API can I use to extract text data from images such as: nutrition labels, ingredient lists, cashier receipts, etc? Thank you.

The Vision framework offers the RecognizeDocumentRequest which is specifically designed for these use cases. It not only recognizes text in images but also provides the structure of the document, such as rows in a receipt or the layout of a nutrition label. It can also identify data like phone numbers, addresses, and prices.

What is the context window for the model? What are max tokens in and max tokens out?

The context window for the Foundation Model is 4,096 tokens. The split between input and output tokens is flexible. For example, if you input 4,000 tokens, you'll have 96 tokens remaining for the output. The API takes in text, converting it to tokens under the hood. When estimating token count, a good rule of thumb is 3-4 characters per token for languages like English, and 1 character per token for languages like Japanese or Chinese. Handle potential errors gracefully by asking for shorter prompts or starting a new session if the token limit is exceeded.

Is there a rate limit for Foundation Models API that is limited by power or temperature condition on the iPhone?

Yes, there are rate limits, particularly when your app is in the background. A budget is allocated for background app usage, but exceeding it will result in rate-limiting errors. In the foreground, there is no rate limit unless the device is under heavy load (e.g., camera open, game mode). The system dynamically balances performance, battery life, and thermal conditions, which can affect the token throughput. Use appropriate quality of service settings for your tasks (e.g., background priority for background work) to help the system manage resources effectively.

Do the foundation models support languages other than English?

Yes, the on-device Foundation Model is multilingual and supports all languages supported by Apple Intelligence. To get the model to output in a specific language, prompt it with instructions indicating the user's preferred language using the locale API (e.g., "The user's preferred language is en-US"). Putting the instructions in English, but then putting the user prompt in the desired output language is a recommended practice.

Are larger server-based models available through Foundation Models?

No, the Foundation Models API currently only provides access to the on-device Large Language Model at the core of Apple Intelligence. It does not support server-side models. On-device models are preferred for privacy and for performance reasons.

Is it possible to run Retrieval-Augmented Generation (RAG) using the Foundation Models framework?

Yes, it is possible to run RAG on-device, but the Foundation Models framework does not include a built-in embedding model. You'll need to use a separate database to store vectors and implement nearest neighbor or cosine distance searches. The Natural Language framework offers simple word and sentence embeddings that can be used. Consider using a combination of Foundation Models and Core ML, using Core ML for your embedding model.

Can developers train their own adapters (LoRAs) to be used with the Foundation models?

Yes, Apple has released a Foundation Models Adapter Training for ML practitioners to train custom adapters. This is useful for specialized datasets and can reduce the amount of prompting needed. However, you are responsible for retraining the adapter whenever Apple updates the base model in a new OS. Adapters are approximately 150-160 MB in size, and can be integrated using the background asset delivery APIs.

Can you talk a bit about the improvements that were made this year in existing Vision Models?

Improvements have been made increasing accuracy and speed. A new RecognizeDocumentRequest has been added for analyzing documents like receipts and nutrition labels. There are also new lens smudge detection and hand pose models.

I’m developing an educational app about some ancient books, how can how can I load all the books’ text into an Apple model?

Due to the context window limit, you'll need to get creative. Consider chunking the text into windows (potentially overlapping), using recursive summarization to condense the information, or employing search algorithms to find relevant passages and only load those. Also consider what the user interaction is and if tool calling can be used to pull out relevant sections as needed. If needed, MLX supports very large context sizes, on the order of millions of tokens.

Will the @Generable macro support Date in the near future?

Create your own Date struct and mark it as @Generable. This provides the flexibility to define the date format and precision (e.g., year, month, day, hour, minute, second) that best suits your app's needs. The order of properties in the @Generable struct matters, as it influences the order in which the model generates the output.

At WWDC25 we launched a new type of Lab event for the developer community - Group Labs. A Group Lab is a panel Q&A designed for a large audience of developers. Group Labs are a unique opportunity for the community to submit questions directly to a panel of Apple engineers and designers. Here are the highlights from the WWDC25 Group Lab for Camera & Photos.

WWDC25 Camera & Photos group lab ran for one hour at 6 PM PST on Tuesday June 10th, 2025

Question 24

What’s the best approach for optimizing barcode scanning using AVFoundation or Vision in low-light or angled scenarios

Turn on flash in low-light scenarios
Lower framerate to improve exposure and reduce noise
Wait until the capture is in focus/notify your user that they need to get closer
Question 25

Recent iPhone models introduced macro mode which automatically switch between lenses to take into account of the focal distance difference. Is there official API to implement this, or should I implement them myself using LiDAR values.

Using builtInTripleCamera and builtInDualWideCamera will automatically switch to macro when available
Question 26

Is there a way to quickly create a thumbnail after the user selects an image with PhotosPicker?

File provider API
Additional questions from the WWDC25 in-person labs that occurred later in the WWDC week

Question 1

When should I build my custom photo picker instead of using the system one?

Always start with the system picker -> try embeddable customization APIs -> fallback to custom picker for very special needs
Question 2

I'm building a new camera app for pros and I want to give my users the most un-processed image possible, and the most control over the capture as possible. How can I do that with AVCapture?

If stills, Brief Bayer RAW capture overview, or Pro RAW if you want Apple's processing and dynamic range
If video, talk about prores LOG.
Custom exposure settings are available throguh the apis
maybe global/local tonemapping discussion?


(Note: this is part 2 of a 3 part posting. See Part 1 or Part 3)

At WWDC25 we launched a new type of Lab event for the developer community - Group Labs. A Group Lab is a panel Q&A designed for a large audience of developers. Group Labs are a unique opportunity for the community to submit questions directly to a panel of Apple engineers and designers. Here are the highlights from the WWDC25 Group Lab for Camera & Photos.

WWDC25 Camera & Photos group lab ran for one hour at 6 PM PST on Tuesday June 10th, 2025

Question 10

Can we directly integrate auto-capture triggers (e.g., when image is steady or text is detected) using Vision and AVFoundation?

Yes apps can use AVCaptureSession's VDO + AVCapturePhotoOutput, run vision on VDO buffers and capture photo when certain scene or text is detected.
Just to be careful to run Vision on VDO buffers async so it doesn't cause frame drops.
Question 11

What Camera or Photos framework features support working with images from external media, like connected cameras or SD cards? Any best practices?

The ImageCaptureCore framework supports camera devices, memory cards, scanners
read and write, where supported
check out the docs to see how to browse connected devices, folders, files, etc.
Question 12

Hi Brad, to follow up on your SwiftUI cautionary note: using AVCaptureVideoPreview inside a UIViewRepresentable, is okay, right? Thanks all for the great info!

Yes, this is totally fine.
AppKit or UIKit views inside appropriate SwiftUI representables should be equivalent performance
Question 13

What’s the “right” way to transition media in my photos app between HDR modes? When I’m in a one-up view, we use HDR, but in other contexts (like thumbnail) we don’t want HDR. Is there a nice way to tone map?

There’s a suite of new System Tone Mapper APIs in this years’ OSes
CoreImage ImageKit CoreAnimation, CoreGraphics
For example:
CoreImage: new CISystemToneMap filter.
CoreAnimation: layer.preferredDynamicRange = CADynamicRangeConstrainedHigh
Using image views (NSImageView/UIImageView/SwiftUI Image/CALayer) support animations on preferredDynamicRange
Can go from high to constrained to standard
Tone mapping is provided by the system (CISystemToneMap for controllable example)
Question 14

What is your recommendation to preprocess and upscale your depth map in order to render a realistic portrait mode image?

One way to do this: the CIEdgePreserveUpsample CIFilter can be use to upsample a lower resolution depth map by using a higher resolution RGB image as a guide.
Question 15

For buffering frames for later processing from real-time camera output should we prefer a AVSampleBufferDisplayLayer centered approach or AVCaptureVideoDataOutputSampleBufferDelegate centered approach? When would we use each?

AVSampleBufferDisplayLayer and AVCaptureVideoDataOutputSampleBufferDelegate are used hand in hand for custom camera preview.
For buffering for later processing, ensure you make copies of VDO buffers to not drop frames from the output
Question 16

Hello, my question is on Deferred Photo Processing? Say I have a photo capture app that adds a CIFilter to the capture. How can I take advantage of Deferred Photo Processing? Since I don’t know how to detect when the deferred captured photo is ready

CIFilter can be called on the final at that point
Photo will have to be re-inserted into the Photo library as adjustment
Question 17

Is digital zoom (e.g., 1.5x) before taking a photo the same as cropping the photo afterward?

digital zoom upscales the image to output dimensions and cropping will yield a smaller output image
while digital zoom will crop, it also upscales
Question 18

How do you design camera interfaces that work for both casual users and photography enthusiasts?

Progressive disclosure: Put the most common controls up front, and make it easy for pros to drill down.
Sensible Defaults: Choose defaults that work well for casual users, but allow those defaults to be modified for photography enthusiasts
A good philosophy is: Keep the simple things easy, make the hard things possible
Question 19

Recent iPhone models introduced macro mode which automatically switch between lenses to take into account of the focal distance difference. Is there official API to implement this, or should I implement them myself using LiDAR values.

Using builtInTripleCamera and builtInDualWideCamera will automatically switch to macro when available
Question 20

a couple of years ago at WWDC, the option of replacing a camera with a virtual camera was mentioned. How does one do that - make the “physical” camera effectively disappear, so only the virtual camera is accessible to the user?

You can't prevent the built-in camera from being available to the user
Question 21

Can developers now integrate custom Core ML models with Vision for on-device photo analysis more seamlessly?

Yes they can, use CoreMLRequest , provide their model container
Been supported for a while (iOS 18/macOS 15)
For more details go to Machine Learning & AI group lab Thursday
use smaller images for better performance
Question 22

What would you recommend for capture of the new immersive and spatial formats?

To capture Spatial Video use AVCaptureMovieFileOutput’s spatialVideoCaptureEnabled property
Not all device formats support spatial capture, check AVCaptureDevice.activeFormat.spatialVideoCaptureSupported
See WWDC 2024 talk “Build compelling spatial photo and video experiences” for more details
Question 23

You mentioned JPEG-XL. What is the current status of support on iOS and macOS for encoding and decoding?

For decoding, we support JPEG-XL files in all our OSes, regular SDR files, as well as ISO HDR files.
For encoding, we only support JPEG-XL for ProRAW DNG capture in the Camera app or via third-party AVFoundation APIs.
If you have any requests for improvement or new features related to JPEG-XL, please file a Feedback request using the Feedback Assistant.
(Note: this is part 2 of a 3 part posting. See Part 1 or Part 3)


Hello,

Thank you for attending today’s Metal & game technologies group lab at WWDC25!

We were delighted to answer many questions from developers and energized by the community engagement. We hope you enjoyed it and welcome your feedback.

We invite you to carry on the conversation here, particularly if your question appeared in Slido and we were unable to answer it during the lab.

If your question received feedback let us know if you need clarification.

You may want to ask your question again in a different lab e.g. visionOS tomorrow. (We realize that this can be confusing when frameworks interoperate)

We have a lot to learn from each other so let’s get to Q&A and make the best of WWDC25! 😃

Looking forward to your questions posted in new threads.


If Getting into visionOS and metal for creating games - Is there any tips or optimal apis/framework/tools that you would recommend?

Easiest way to start on visionOS is with RealityKit. The GameController framework supports lots of controllers for different types of games. 200GB of Background Assets available for content rich games.

Are there standard methods of achieving synchronization of state between players (specifically using VisionOS here) or is it a roll-your-own situation?

For synchronizing objects in a Shared spatial environment, consider using TableTopKit, which is compatible with GKMatch in the GameKit framework. If you’re using RealityKit, you can use SynchronizationService to coordinate shared physically simulated objects. We also recommend the WWDC session: Share visionOS experiences with nearby people.

Which official Metal docs or sample projects best support teams moving a CPU-centric CAD kernel to GPU-accelerated workflows on Apple Silicon

The Performing Calculations on a GPU sample code shows how to use Metal to identify available GPUs and run calculations on them.

Processing a Texture in a Compute Function sample code demonstrates creating textures by running copy and dispatch commands within a compute pass on the GPU.

Is there a way to automatically extract Metal performance metrics (e.g. to a CSV file) from a GPU trace for further analysis outside of the metal debugger?

You can capture a gputrace and replay it with profiling. Go to Performance and select the Xcode → Editor → Export GPU Counters option to export the performance metrics as a CSV file.

How can Game Center be used to with SwiftData apps to determine when someone has earned an award (ex. a task tracking app that wants to give out awards for completing tasks)?

GameKit provides APIs for querying the player’s earned achievements and leaderboards. You can read from those APIs and write to your own SwiftData or other persistence system to trigger rewards in your game. Generally speaking, Game Center does not directly interact with SwiftData so it’s easiest to use GameKit APIs to post scores and achievement progress in parallel with your own database or backend services.

Hey, Thank you so much for your work and this session! I have a quick question regarding the Games app. When we publish new games on the App Store, are they automatically added to the Games app as well? Is there any special integration required for this process?

If the app is set to the Games category, it will show up in the Games app automatically without extra work. If you adopt Game Center it will also appear in the games library. Most importantly, the more Game Center features the game adopts, the more the OS knows about it and the more places it can be featured in the Games app. Adopting Game Center makes your game eligible for the Top Played Games chart. This chart measures play time where play time is measured starting from when your game initializes Game Center.

Is there a way to do a local p2p network based game? I am trying to find a way to create a coop party game, which doesn’t require people to be connected to internet.

For fully offline play, use Multipeer Connectivity or the Network framework. You can use Multipeer Connectivity or Network framework to create a local P2P network-based game without requiring Internet. For a local peer-to-peer games, Game Center’s Nearby Multiplayer offers useful features but requires an Internet connection.


How development for VisionOS can benefit of Metal 4?

You can develop in Metal, as there is template code available for it. Metal 4 can help reduce the CPU overheads for encoding rendering, and VisionOS supports presenting from Metal 4 command queues.

The new Metal 4 API looks like it opens some more possibilities for threaded encoding. I also noticed there wasn’t much mention of GPU driven encoding this year. Is GPU driven encoding or CPU driven encoding the best starting point for building a performant Metal app?

Metal 3 and 4 support the same GPU-driven rendering features (ICE).

Concurrent CPU encoding is typically a good starting point for improving performance through parallelism, and three threads is a good match for a standard Metal drawable pool.

Metal 4 makes concurrent CPU encoding easier through explicit scheduling.

What scheduling pattern is advised to run long compute passes (e.g. topology optimization) alongside interactive rendering in one Metal app without hurting UI latency?

Use a separate queue for issuing compute long running compute dispatches so that interactive rendering can run concurrently and use Metal events for synchronizing data dependencies between command queues.

How do you think about the Games app compared to the previously removed Game Center app? What aspects and integrations do you think will keep players coming back in a way they didn't for the original app despite it also having challenges? Should we be pushing people to use the app from our games?

Game Center itself has had a series of improvements for developers and players: activity feed, widgets, integrations into the App Store, improvements to friending and the social graph, multiplayer matchmaking, and more.

The new Games app brings all of these Game Center capabilities together in one place that players can easily find. It also offers deeper game discovery through search and personalized recommendations. From within your games, you can guide users into the Games app if you’d like, you also have the option to bring them into the Game Overlay so they don’t need to leave your game experience.

The more you integrate features like Challenges, the more prominently your game will appear within the Games app, which can help keep players engaged in ways the original Game Center app did not.

Is there a way to configure challenges for all my existing leaderboards in bulk? Looks like this will be something most game developers will want to do. Especially, have a challenge for each level in the game.

The new GameKit bundle in Xcode is recommended as it provides UI for configuring these elements in Xcode. Under the hood, the GameKit bundle is a JSON file, which can be easily automated with a script. You can also checkout the WWDC session: Get started with Game Center for best practices in configuring challenges.

Are there any actual game sample projects available to download?

Metal Sample Code Library
Try Bring your advanced games to Mac, iPad, and iPhone. This tutorial covers essential technologies for implementing games on Apple platforms, including Metal rendering, audio, input, and display APIs.
See Port advanced games to Apple platforms
See Design advanced games for Apple platforms

Inspired by Liquid Glass, I want to add shader effects to my app in SwiftUI, but I'm concerned about performance. How do Metal shaders fit in to the SwiftUI layout/drawing system, and are there any performance pitfalls to watch out for?

SwiftUI invokes your shaders during rendering. We recommend Optimize SwiftUI performance with Instruments to understand the performance impact of Metal shaders executed during a SwiftUI rendering pass.

A newbie to Apple Developer Experience - where can I start if i want to start learning to create games (i understand it will take time)

A good place to get started is the Game Porting Toolkit. This resource covers essential technologies for implementing games on Apple platforms, including Metal rendering, audio, input, and display APIs.

0

Note: this is part 1 of a 3 part posting. See Part 2 or Part 3)

At WWDC25 we launched a new type of Lab event for the developer community - Group Labs. A Group Lab is a panel Q&A designed for a large audience of developers. Group Labs are a unique opportunity for the community to submit questions directly to a panel of Apple engineers and designers. Here are the highlights from the WWDC25 Group Lab for Camera & Photos.

WWDC25 Camera & Photos group lab ran for one hour at 6 PM PST on Tuesday June 10th, 2025

Introductory kick-off questions

Question 1

Tell us a little about the new AVFoundation Capture APIs we've made available in the new iOS 26 developer preview?

Cinematic Capture API (strong/weak focus, tracking focus)(scene monitoring)(simulated aperture)(dog/cat heads/groupIDs)
Camera Controls and AirPod Stem Clicks
Spatial Audio and Studio Quality AirPod Mics in Camera
Lens Smudge Detection
Exposure and Focus Rect of Interest
Question 2

I built QR code scanning into my app, but on newer iPhones I have to hold the phone very far away from the QR code, otherwise the image is blurry and it doesn't scan. Why is this happening and how can I fix it?

Every year, the cameras get better and better as we push the state of the art on iPhone photography and videography. This sometimes results in changes to the characteristics of the lenses.
min focus distance
newer phones have multiple lenses
automatic switching behavior
Use virtual device like the builtInDualWide or built in Triple, rather than just the builtInWide
Set the videoZoomFactor to 2. You're done.
Question 3

Last year, we saw some exciting new APIs introduced in AVFoundation in the health space. With Constant Color photography, developers can take pictures that have constant color regardless of ambient lighting. There are some further advancements this year. Davide, could you tell us about them?

constant color photography is mean to remove the "tone mapping" applied to photograph captured with camera app, usually incldsuing artistic intent, and instead try to be a close as possible to the real color of the scene, regardless of the illumination
constant color images could be captured in HEIF anf JPEG laste year. this year we are adding Support for the DICOM medical imaging photo format. It is a fomrat used by the health industry to store images related to medical subjects like MRI, skin problems, xray and so on.
It's writable and also readable format on all OS26, supported through AVCapturePhotoOutput APIs and through the coregraphics api.
for coregrapphics there is a new DICOM entry in the property dictionary which includes all the dicom availbale and defined propertie in a file. finder will also display all those in the info panel
(Address why a developer would want to use it) - not for regualr picture taking apps. for those HEIF and JPEG are the preferred delivery format. use dicom if your app produces output that are health related, that you can also share with health providers or your doctors
Main session developer questions

Question 1

LiDAR vs. Dual Camera depth generation: Which resolution does the LiDAR sensor natively have (iPhone 16 Pro) and when to prefer LiDAR over Dual Camera?

Both report formats with output resolutions (we don't advertise sensor resolution)
Lidar vs Dual, etc:
Lidar: Best for absolute depth, real world scale and computer vision
Dual, etc: relative, disparity-based, less power, photo effects
Also see: 2022 WWDC session "Discovery advancements in iOS camera capture: Depth, focus and multitasking"
Question 2

Can true depth and lidar camera run at 60fps?

Lidar can do 30fps (edited)
Front true depth can do 60fps.
Question 3

What’s the first class way to use PhotoKit to reimplement a high performance photo grid? We’ve been using a LazyVGrid and the photos caching manager, but are never able to hit the holy trinity (60hz, efficient memory footprint, minimal flashes of placeholder/empty cells)

use the PHCachingImageManager to get media content delivered before you need to display it
specify the size you need for grid sized display
set the options PHVideoRequestOptionsDeliveryModeFastFormat, PHImageRequestOptionsDeliveryModeFastFormat and PHImageRequestOptionsResizeModeFast
Question 4

For rending live preview of video stream, Is there performance overhead from using async and Swift UI for image updates vs UIViewRepresentable + AVCaptureVideoPreviewLayer.self?

AVCaptureVideoPreviewLayer is the most efficient display path
Use VDO + AVSampleBufferDisplayLayer if you need to modify the image data
Swift UI image is optimized for static image content
Question 5

Is there a way to configure the AVFoundation BuiltInLiDarDepthCamera mode to provide a depth map as accurate as ARKit at close range?

The AVCaptureDepthDataOutput supports filtering that reduces noise and fills in invalid values. Consider using this for smoother depth maps
Question 6

Pyramid-based photo editing in core image (such as adobe camera raw highlights and shadows)?

First off you may want to look a the builtin filter called CIHighlightShadowAdjust
Also the noise reduction in the CIRawFilter uses a pyramid-based algorithm.
You can also write your own pyramid-based algorithms by taking an input image:
down sample it by two multiply times using imageByApplyingAffineTransform
apply additional CIKernels to each downsampled image as needed.
use a custom CIKernel to combine the results.
Question 7

Is the best way to integrate an in-app camera for a “non-camera” app UIImagePickerController?

Yes, UIImagePickerController provides system-provided UI for capturing photos and movies.
Question 8

Hello, my question is on Deferred Photo Processing? Say I have a photo capture app that adds a CIFilter to the capture. How can I take advantage of Deferred Photo Processing? Since I don’t know how to detect when the deferred captured photo is ready

CIFilter can be called on the final at that point
Photo will have to be re-inserted into the Photo library as adjustment
Question 9

For shipping photo style assets in the app that need transparency what is the best format to use? JPEG2000? will moving to this save a lot of space comapred to PNG or other options?

If you want lossless compression PNG is good and supports unpremutiplied alpha
If you want lossy compression HEIF supports premutiplied or unpremutiplied alpha
(Note: this is part 1 of a 3 part posting. See Part 2 or Part 3)